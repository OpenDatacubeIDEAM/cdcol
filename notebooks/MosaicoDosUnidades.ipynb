{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "execID=23\n",
    "algorithm = \"MosaicoDosUnidades\"\n",
    "version= \"1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = ['LS5_TM_LEDAPS','LS7_ETM_LEDAPS' ] #Productos sobre los que se hará la consulta (unidades de almacenamiento)\n",
    "bands=[\"blue\",\"green\",\"red\",\"nir\", \"swir1\",\"swir2\"] #arreglo de bandas \n",
    "time_ranges = [(\"2015-04-01\", \"2015-06-30\")] #Una lista de tuplas, cada tupla representa un periodo\n",
    "#área sobre la cual se hará la consulta:\n",
    "min_long = -71\n",
    "min_lat = -5\n",
    "max_long = -69\n",
    "max_lat = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros específicos del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized=True\n",
    "minValid=1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.storage import netcdf_writer\n",
    "from datacube.model import Variable, CRS\n",
    "import os\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gdal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta sobre las diferentes unidades y aplica la máscara de nubes adecuada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodata=-9999\n",
    "#Definir las funciones necesatias para el algoritmo\n",
    "def isin(element, test_elements, assume_unique=False, invert=False):\n",
    "    \"definiendo la función isin de numpy para la versión anterior a la 1.13, en la que no existe\"\n",
    "    element = np.asarray(element)\n",
    "    return np.in1d(element, test_elements, assume_unique=assume_unique,\n",
    "                invert=invert).reshape(element.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máscara de nubes\n",
    "\n",
    "Con el nuevo formato, los valores de `pixel_qa` dependen del producto. Para crear la máscara de nubes, se determinan los valores válidos para el producto actual y se usa la banda `pixel_qa` para generar un arreglo de datos booleanos: Para cada posición, si el valor de pixel_qa está en la lista de valores válidos será `True`, en caso contrario será `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kwargs={}\n",
    "dc = datacube.Datacube(app=\"{}_{}_{}\".format(algorithm,version,execID))\n",
    "for product in products:\n",
    "    i=0\n",
    "    validValues=set()\n",
    "    if product==\"LS5_TM_LEDAPS\":\n",
    "        validValues=[66,68,130,132]\n",
    "    elif product == \"LS7_ETM_LEDAPS\":\n",
    "        validValues=[322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "    for tr in time_ranges:\n",
    "        _data = dc.load(product=product, longitude=(min_long, max_long), latitude=(min_lat, max_lat), time=tr)\n",
    "        if len(_data.data_vars)==0:\n",
    "            break\n",
    "        cloud_mask=isin(_data[\"pixel_qa\"].values, validValues)\n",
    "        for band in bands:\n",
    "            _data[band].values=np.where(np.logical_and(_data.data_vars[band]!=nodata,cloud_mask),_data.data_vars[band], np.nan)\n",
    "        _undesired=list(set(_data.keys())-set(bands+['latitude','longitude','time']))\n",
    "        _data=_data.drop(_undesired)\n",
    "            \n",
    "        if \"xarr\"+str(i) in kwargs:\n",
    "            kwargs[\"xarr\"+str(i)]=xr.concat([kwargs[\"xarr\"+str(i)],_data.copy(deep=True)], 'time')\n",
    "        else:\n",
    "            kwargs[\"xarr\"+str(i)]=_data\n",
    "    i+=1\n",
    "del _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_undesired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#El algoritmo recibe los productos como xarrays en variablles llamadas xarr0, xarr1, xarr2... \n",
    "xarr0=kwargs[\"xarr0\"]\n",
    "del kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xarr0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo debe ser auto contenido. Puede importar y usar las librerías disponibles en CDCol, por ejemplo: \n",
    "\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- xarray\n",
    "- pycurl\n",
    "- pandas\n",
    "- nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compuesto temporal de medianas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians={} \n",
    "for band in bands:\n",
    "    datos=xarr0[band].values\n",
    "    allNan=~np.isnan(datos) #Una mascara que indica qué datos son o no nan. \n",
    "    if normalized: #Normalizar, si es necesario.\n",
    "        #Para cada momento en el tiempo obtener el promedio y la desviación estándar de los valores de reflectancia\n",
    "        m=np.nanmean(datos.reshape((datos.shape[0],-1)), axis=1)\n",
    "        st=np.nanstd(datos.reshape((datos.shape[0],-1)), axis=1)\n",
    "        # usar ((x-x̄)/st) para llevar la distribución a media 0 y desviación estándar 1, \n",
    "        # y luego hacer un cambio de espacio para la nueva desviación y media. \n",
    "        datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "    #Calcular la mediana en la dimensión de tiempo \n",
    "    medians[band]=np.nanmedian(datos,0) \n",
    "    #Eliminar los valores que no cumplen con el número mínimo de pixeles válidos dado. \n",
    "    medians[band][np.sum(allNan,0)<minValid]=np.nan\n",
    "del datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_coords=xarr0.coords\n",
    "_crs=xarr0.crs\n",
    "del xarr0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar la salida\n",
    "La salida de los algoritmos puede expresarse como: \n",
    "- un xarray llamado output (que debe incluir entre sus atributos el crs del sistema de coordenadas)\n",
    "- un diccionario con varios xarray llamado `outputs`\n",
    "- Texto, en una variable llamada `outputtxt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in _coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, _coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=_coords[x]\n",
    "variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords)\n",
    "             for k, v in medians.items()}\n",
    "output=xr.Dataset(variables, attrs={'crs':_crs})\n",
    "\n",
    "for x in output.coords:\n",
    "    _coords[x].attrs[\"units\"]=_coords[x].units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar la salida\n",
    "La tarea genérica se encarga de generar los archivos de salida en la carpeta adecuada. \n",
    "\n",
    "__Nota__: A diferencia de la tarea genérica, que maneja los 3 tipos de salida descritos en la sección anterior, este cuaderno sólo guarda la salida definida en output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.storage import netcdf_writer\n",
    "from datacube.model import Variable, CRS\n",
    "print \"{}_{}_{}.nc\".format(algorithm,version,execID)\n",
    "nco=netcdf_writer.create_netcdf(\"{}_{}_{}.nc\".format(algorithm,version,execID))\n",
    "cords=('latitude', 'longitude','time')\n",
    "for x in cords:\n",
    "    if(x!=\"time\"):\n",
    "        netcdf_writer.create_coordinate(nco, x, _coords[x].values, _coords[x].units)\n",
    "netcdf_writer.create_grid_mapping_variable(nco, _crs)\n",
    "for band in bands:\n",
    "    medians[band][np.isnan(medians[band])]=nodata\n",
    "    var= netcdf_writer.create_variable(nco, band, Variable(np.dtype(np.int32), None, ('latitude', 'longitude'), None) ,set_crs=True)\n",
    "    var[:] = netcdf_writer.netcdfy_data(medians[band])\n",
    "nco.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
