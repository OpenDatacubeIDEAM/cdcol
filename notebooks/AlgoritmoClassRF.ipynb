{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo para clasificación de Palma - CDCol IDEAM\n",
    "\n",
    "El flujo metodológico para la elaboración del mapa nacional de cobertura de palma de aceite 2017 se encuentra desarrollado sobre un algoritmo semi-automatizado de aprendizaje maquina (Random Forest) diseñado haciendo uso de la arquitectura del Cubo de datos para Colombia (CDCol) del IDEAM y escrito en lenguaje Python 2.7.\n",
    "\n",
    "El conjunto de datos para entrenamiento corresponde a la elaboración de compuestos temporales de mediana (CTM) anuales para el año 2017, usando cinco bandas espectrales (Green, Reed, NIR, SWIR1 y SWIR2), elaborados de dos unidades de almacenamiento (Landsat 7 y Landsat 8). Sobre los CTM obtenidos se calcularon tres (3) índices temáticos que aportaron una mejor respuesta a la clasificación de la cobertura. Dentro de los índices seleccionados se encuentran el NDVI (Normalized Difference Vegetation Index) que permite separar la vegetación de los cultivos del brillo que produce el suelo, el GNDVI (Green NDVI) un índice variante del NDVI y permite establecer la diferencia la vegetación y el estado del suelo y el RVI (Ratio Vegetation Index) el cual permite determinar la diferencia entre la vegetación y as características del suelo. Se optó por la inclusión en el compuesto del DEM para entrenar el algoritmo ya que de acuerdo con el ambiente de siembra de los cultivos de Palma de Aceite las condiciones óptimas para el cultivo se requiere una altitud máxima de 500 msnm con pendientes menores a 23%, los suelos deben ser planos o con ondulamiento suave, suelos francos y con buen drenaje  (Mingorance, Minelli, & Du, 2004), por lo que esta variable brindaba aporte en el entrenamiento. La clasificación se realizó en las zonas libres del enmascaramiento de los pixeles correspondientes a cobertura boscosa, como mascara se usó el mapa oficial de Bosque / No Bosque 2017 elaborado por el IDEAM.\n",
    "\n",
    "Las clases finales del mapa corrsponden a un mapa tematico de Palma/ No palma. El mapa se presenta en escala 1: 100.000, por lo que el mosaico final esta generalizado a la unidad minima de mapeo (UMM) de 1 ha. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo desarrollado en python 2.7, en la infraestructura desacoplado del Cubo de Datos de Imágenes de Satélite de Colombia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "execID=975\n",
    "algorithm = \"RandomForestCm\"\n",
    "version= \"1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = ['LS8_OLI_LASRC','LS7_ETM_LEDAPS' ] #Productos sobre los que se hará la consulta (unidades de almacenamiento)\n",
    "bands=[\"green\",\"red\",\"nir\", \"swir1\",\"swir2\"] #arreglo de bandas #\"blue\",\"green\",\n",
    "time_ranges = [(\"2017-01-01\", \"2017-12-31\")] #Una lista de tuplas, cada tupla representa un periodo\n",
    "#área sobre la cual se hará la consulta:\n",
    "min_long = -75\n",
    "min_lat = 9\n",
    "max_long = -74\n",
    "max_lat = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros específicos del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = '/home/cubo/notebooks/975'\n",
    "validation_data_path= '/home/cubo/notebooks/975'\n",
    "normalized=True\n",
    "minValid=1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.storage import netcdf_writer\n",
    "from datacube.model import Variable, CRS\n",
    "import os\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gdal\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enmascarar_entrenamiento(vector_data_path, cols, rows, geo_transform, projection, target_value=1):\n",
    "    data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)\n",
    "    layer = data_source.GetLayer(0)\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    target_ds = driver.Create('', cols, rows, 1, gdal.GDT_UInt16)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "    gdal.RasterizeLayer(target_ds, [1], layer, burn_values=[target_value])\n",
    "    return target_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rasterizar_entrenamiento(file_paths, rows, cols, geo_transform, projection):\n",
    "    labeled_pixels = np.zeros((rows, cols))\n",
    "    for i, path in enumerate(file_paths):\n",
    "        label = i+1\n",
    "        ds = enmascarar_entrenamiento(path, cols, rows, geo_transform, projection, target_value=label)\n",
    "        band = ds.GetRasterBand(1)\n",
    "        labeled_pixels += band.ReadAsArray()\n",
    "        ds = None\n",
    "    return labeled_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exportar(fname, data, geo_transform, projection):\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta sobre las diferentes unidades y aplica la máscara de nubes adecuada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodata=-9999\n",
    "#Definir las funciones necesatias para el algoritmo\n",
    "def isin(element, test_elements, assume_unique=False, invert=False):\n",
    "    \"definiendo la función isin de numpy para la versión anterior a la 1.13, en la que no existe\"\n",
    "    element = np.asarray(element)\n",
    "    return np.in1d(element, test_elements, assume_unique=assume_unique,\n",
    "                invert=invert).reshape(element.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máscara de nubes\n",
    "\n",
    "Con el nuevo formato, los valores de `pixel_qa` dependen del producto. Para crear la máscara de nubes, se determinan los valores válidos para el producto actual y se usa la banda `pixel_qa` para generar un arreglo de datos booleanos: Para cada posición, si el valor de pixel_qa está en la lista de valores válidos será `True`, en caso contrario será `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cubo/anaconda2/lib/python2.7/_abcoll.py:410: FutureWarning: iteration over an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Iterate over the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  return list(self)\n",
      "/home/cubo/anaconda2/lib/python2.7/_abcoll.py:410: FutureWarning: calling len() on an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Call len() on the Dataset.variables property instead, like ``len(ds.variables)``, to preserve existing behavior in a forwards compatible manner.\n",
      "  return list(self)\n"
     ]
    }
   ],
   "source": [
    "kwargs={}\n",
    "dc = datacube.Datacube(app=\"{}_{}_{}\".format(algorithm,version,execID))\n",
    "for product in products:\n",
    "    i=0\n",
    "    validValues=set()\n",
    "    if product==\"LS7_ETM_LEDAPS\":\n",
    "        validValues=[66,68,130,132]\n",
    "    elif product == \"LS8_OLI_LASRC\":\n",
    "        validValues=[322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "    for tr in time_ranges:\n",
    "        _data = dc.load(product=product, longitude=(min_long, max_long), latitude=(min_lat, max_lat), time=tr)\n",
    "        if len(_data.data_vars)==0:\n",
    "            break\n",
    "        cloud_mask=isin(_data[\"pixel_qa\"].values, validValues)\n",
    "        for band in bands:\n",
    "            _data[band].values=np.where(np.logical_and(_data.data_vars[band]!=nodata,cloud_mask),_data.data_vars[band], np.nan)\n",
    "        _undesired=list(set(_data.keys())-set(bands+['latitude','longitude','time']))\n",
    "        _data=_data.drop(_undesired)\n",
    "            \n",
    "        if \"xarr\"+str(i) in kwargs:\n",
    "            kwargs[\"xarr\"+str(i)]=xr.concat([kwargs[\"xarr\"+str(i)],_data.copy(deep=True)], 'time')\n",
    "        else:\n",
    "            kwargs[\"xarr\"+str(i)]=_data\n",
    "    i+=1\n",
    "del _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'blue',\n",
       " u'solar_azimuth',\n",
       " u'sensor_zenith',\n",
       " u'radsat_qa',\n",
       " u'atmos_opacity',\n",
       " u'solar_zenith',\n",
       " u'pixel_qa',\n",
       " u'cloud_qa',\n",
       " u'sensor_azimuth']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_undesired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El algoritmo recibe los productos como xarrays en variablles llamadas xarr0, xarr1, xarr2... \n",
    "xarr0=kwargs[\"xarr0\"]\n",
    "del kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 3687, longitude: 3704, time: 147)\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float64 10.0 10.0 10.0 9.999 9.999 9.999 9.998 ...\n",
       "  * longitude  (longitude) float64 -75.0 -75.0 -75.0 -75.0 -75.0 -75.0 -75.0 ...\n",
       "  * time       (time) datetime64[ns] 2017-01-09T15:11:20 2017-01-09T15:11:44 ...\n",
       "Data variables:\n",
       "    green      (time, latitude, longitude) float64 nan nan nan nan nan nan ...\n",
       "    red        (time, latitude, longitude) float64 nan nan nan nan nan nan ...\n",
       "    nir        (time, latitude, longitude) float64 nan nan nan nan nan nan ...\n",
       "    swir1      (time, latitude, longitude) float64 nan nan nan nan nan nan ...\n",
       "    swir2      (time, latitude, longitude) float64 nan nan nan nan nan nan ...\n",
       "Attributes:\n",
       "    crs:      EPSG:4326"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xarr0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo debe ser auto contenido. Puede importar y usar las librerías disponibles en CDCol, por ejemplo: \n",
    "\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- xarray\n",
    "- pycurl\n",
    "- pandas\n",
    "- nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"{}_{}_{}\".format(algorithm,version,execID))\n",
    "\n",
    "_data = dc.load(product=\"FNF_COL_UTM\", longitude=(min_long, max_long), latitude=(min_lat, max_lat), time=(\"2017-01-01\", \"2017-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 3687, longitude: 3704, time: 1)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 2017-01-01\n",
       "  * latitude   (latitude) float64 10.0 10.0 10.0 9.999 9.999 9.999 9.998 ...\n",
       "  * longitude  (longitude) float64 -75.0 -75.0 -75.0 -75.0 -75.0 -75.0 -75.0 ...\n",
       "Data variables:\n",
       "    fnf_mask   (time, latitude, longitude) int8 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ...\n",
       "Attributes:\n",
       "    crs:      EPSG:4686"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_data2 = dc.load(product=\"DEM_Mosaico\", longitude=(min_long, max_long), latitude=(min_lat, max_lat), time=(\"2013-01-01\", \"2013-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dem=_data2[\"dem\"][0].values\n",
    "del _data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compuesto temporal de medianas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cubo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning: Mean of empty slice\n",
      "/home/cubo/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1423: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "medians={} \n",
    "for band in bands:\n",
    "    datos=xarr0[band].values\n",
    "    allNan=~np.isnan(datos) #Una mascara que indica qué datos son o no nan. \n",
    "    if normalized: #Normalizar, si es necesario.\n",
    "        #Para cada momento en el tiempo obtener el promedio y la desviación estándar de los valores de reflectancia\n",
    "        m=np.nanmean(datos.reshape((datos.shape[0],-1)), axis=1)\n",
    "        st=np.nanstd(datos.reshape((datos.shape[0],-1)), axis=1)\n",
    "        # usar ((x-x̄)/st) para llevar la distribución a media 0 y desviación estándar 1, \n",
    "        # y luego hacer un cambio de espacio para la nueva desviación y media. \n",
    "        datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "    #Calcular la mediana en la dimensión de tiempo \n",
    "    medians[band]=np.nanmedian(datos,0) \n",
    "    #Eliminar los valores que no cumplen con el número mínimo de pixeles válidos dado. \n",
    "    medians[band][np.sum(allNan,0)<minValid]=np.nan\n",
    "    medians[band][_data[\"fnf_mask\"].values[0]==1]=np.nan\n",
    "del datos\n",
    "del _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medians[\"dem\"]=dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(medians[\"dem\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medians[\"ndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medians[\"gndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"green\"],medians[\"nir\"]+medians[\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medians[\"rvi\"]=np.true_divide(medians[\"nir\"],medians[\"red\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#medians[\"nbr\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#medians[\"nbr2\"]=np.true_divide(medians[\"swir1\"]-medians[\"swir2\"],medians[\"swir1\"]+medians[\"swir2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#medians[\"ndmi\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bands=medians.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_coords=xarr0.coords\n",
    "_crs=xarr0.crs\n",
    "del xarr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(train_data_path) if f.endswith('.shp')]\n",
    "classes = [f.split('.')[0] for f in files]\n",
    "shapefiles = [os.path.join(train_data_path, f) for f in files if f.endswith('.shp')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/cubo/notebooks/975/Palma_975.shp',\n",
       " '/home/cubo/notebooks/975/No_Palma_975.shp']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows, cols = medians[bands[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(originX, pixelWidth, 0, originY, 0, pixelHeight)\n",
    "geo_transform=(_coords[\"longitude\"].values[0], 0.000269995,0, _coords[\"latitude\"].values[0],0,-0.000271302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proj=_crs.wkt\n",
    "#proj='GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_pixels = rasterizar_entrenamiento(shapefiles, rows, cols, geo_transform, proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_train = np.nonzero(labeled_pixels)\n",
    "training_labels = labeled_pixels[is_train]\n",
    "nmed=None\n",
    "bands_data=[]\n",
    "for band in bands: \n",
    "    bands_data.append(medians[band])\n",
    "bands_data = np.dstack(bands_data)\n",
    "#training_samples = nmed[is_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows, cols, n_bands = bands_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_train = np.nonzero(labeled_pixels)\n",
    "training_labels = labeled_pixels[is_train]\n",
    "training_samples = bands_data[is_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_msk=np.sum(np.isfinite(training_samples),1)>1\n",
    "training_samples= training_samples[_msk,:]\n",
    "training_labels=training_labels[_msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2672, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_jobs=-1, n_estimators=500, verbose=1, criterion=\"entropy\" )\n",
    "classifier.fit(training_samples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelo975']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier,'modelo975')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = rows*cols\n",
    "flat_pixels = bands_data.reshape((n_samples, n_bands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13656648,)\n"
     ]
    }
   ],
   "source": [
    "_msk=np.sum(np.isfinite(flat_pixels),1)>1\n",
    "print _msk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:   10.4s finished\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(flat_pixels[_msk])\n",
    "a=np.empty(rows*cols)\n",
    "a[:]=np.nan\n",
    "a[_msk]=result\n",
    "classification = a.reshape((rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#exportar(\"salida-class.tiff\", classification, geo_transform, proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification[np.isnan(classification)]=nodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar la salida\n",
    "La salida de los algoritmos puede expresarse como: \n",
    "- un xarray llamado output (que debe incluir entre sus atributos el crs del sistema de coordenadas)\n",
    "- un diccionario con varios xarray llamado `outputs`\n",
    "- Texto, en una variable llamada `outputtxt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in _coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, _coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=_coords[x]\n",
    "        \n",
    "variables={}\n",
    "#variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords)\n",
    "#             for k, v in medians.items()}\n",
    "variables[\"classification\"]=xr.DataArray(classification,dims=xdims,coords=ncoords)\n",
    "output=xr.Dataset(variables, attrs={'crs':_crs})\n",
    "\n",
    "for x in output.coords:\n",
    "    _coords[x].attrs[\"units\"]=_coords[x].units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar la salida\n",
    "La tarea genérica se encarga de generar los archivos de salida en la carpeta adecuada. \n",
    "\n",
    "__Nota__: A diferencia de la tarea genérica, que maneja los 3 tipos de salida descritos en la sección anterior, este cuaderno sólo guarda la salida definida en output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestCm_1.0_975.nc\n"
     ]
    }
   ],
   "source": [
    "from datacube.storage import netcdf_writer\n",
    "from datacube.model import Variable, CRS\n",
    "print \"{}_{}_{}.nc\".format(algorithm,version,execID)\n",
    "nco=netcdf_writer.create_netcdf(\"{}_{}_{}.nc\".format(algorithm,version,execID))\n",
    "cords=('latitude', 'longitude','time')\n",
    "for x in cords:\n",
    "    if(x!=\"time\"):\n",
    "        netcdf_writer.create_coordinate(nco, x, _coords[x].values, _coords[x].units)\n",
    "netcdf_writer.create_grid_mapping_variable(nco, _crs)\n",
    "var= netcdf_writer.create_variable(nco, \"classification\", Variable(np.dtype(np.int32), nodata, ('latitude', 'longitude'), None) ,set_crs=True)\n",
    "var[:] = netcdf_writer.netcdfy_data(classification)\n",
    "nco.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
