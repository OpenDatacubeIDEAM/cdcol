{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "execID=21\n",
    "algorithm = \"RandomForestCm\"\n",
    "version= \"1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = ['LS8_OLI_LASRC','LS7_ETM_LEDAPS' ] #Productos sobre los que se hará la consulta (unidades de almacenamiento)\n",
    "bands=[\"blue\",\"green\",\"red\",\"nir\", \"swir1\",\"swir2\"] #arreglo de bandas \n",
    "time_ranges = [(\"2016-01-01\", \"2016-12-31\")] #Una lista de tuplas, cada tupla representa un periodo\n",
    "#área sobre la cual se hará la consulta:\n",
    "min_long = -75\n",
    "min_lat = 10\n",
    "max_long = -74\n",
    "max_lat = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros específicos del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = '/home/cubo/notebooks/Piloto_Palma/train-piloto'\n",
    "validation_data_path= '/home/cubo/notebooks/Piloto_Palma/test-piloto'\n",
    "normalized=True\n",
    "minValid=1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.storage import netcdf_writer\n",
    "from datacube.model import Variable, CRS\n",
    "import os\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gdal\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enmascarar_entrenamiento(vector_data_path, cols, rows, geo_transform, projection, target_value=1):\n",
    "    data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)\n",
    "    layer = data_source.GetLayer(0)\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    target_ds = driver.Create('', cols, rows, 1, gdal.GDT_UInt16)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "    gdal.RasterizeLayer(target_ds, [1], layer, burn_values=[target_value])\n",
    "    return target_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rasterizar_entrenamiento(file_paths, rows, cols, geo_transform, projection):\n",
    "    labeled_pixels = np.zeros((rows, cols))\n",
    "    for i, path in enumerate(file_paths):\n",
    "        label = i+1\n",
    "        ds = enmascarar_entrenamiento(path, cols, rows, geo_transform, projection, target_value=label)\n",
    "        band = ds.GetRasterBand(1)\n",
    "        labeled_pixels += band.ReadAsArray()\n",
    "        ds = None\n",
    "    return labeled_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exportar(fname, data, geo_transform, projection):\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta sobre las diferentes unidades y aplica la máscara de nubes adecuada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodata=-9999\n",
    "#Definir las funciones necesatias para el algoritmo\n",
    "def isin(element, test_elements, assume_unique=False, invert=False):\n",
    "    \"definiendo la función isin de numpy para la versión anterior a la 1.13, en la que no existe\"\n",
    "    element = np.asarray(element)\n",
    "    return np.in1d(element, test_elements, assume_unique=assume_unique,\n",
    "                invert=invert).reshape(element.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máscara de nubes\n",
    "\n",
    "Con el nuevo formato, los valores de `pixel_qa` dependen del producto. Para crear la máscara de nubes, se determinan los valores válidos para el producto actual y se usa la banda `pixel_qa` para generar un arreglo de datos booleanos: Para cada posición, si el valor de pixel_qa está en la lista de valores válidos será `True`, en caso contrario será `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs={}\n",
    "dc = datacube.Datacube(app=\"{}_{}_{}\".format(algorithm,version,execID))\n",
    "for product in products:\n",
    "    i=0\n",
    "    validValues=set()\n",
    "    if product==\"LS7_ETM_LEDAPS\":\n",
    "        validValues=[66,68,130,132]\n",
    "    elif product == \"LS8_OLI_LASRC\":\n",
    "        validValues=[322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "    for tr in time_ranges:\n",
    "        _data = dc.load(product=product, longitude=(min_long, max_long), latitude=(min_lat, max_lat), time=tr)\n",
    "        if len(_data.data_vars)==0:\n",
    "            break\n",
    "        cloud_mask=isin(_data[\"pixel_qa\"].values, validValues)\n",
    "        for band in bands:\n",
    "            _data[band].values=np.where(np.logical_and(_data.data_vars[band]!=nodata,cloud_mask),_data.data_vars[band], np.nan)\n",
    "        _undesired=list(set(_data.keys())-set(bands+['latitude','longitude','time']))\n",
    "        _data=_data.drop(_undesired)\n",
    "            \n",
    "        if \"xarr\"+str(i) in kwargs:\n",
    "            kwargs[\"xarr\"+str(i)]=xr.concat([kwargs[\"xarr\"+str(i)],_data.copy(deep=True)], 'time')\n",
    "        else:\n",
    "            kwargs[\"xarr\"+str(i)]=_data\n",
    "    i+=1\n",
    "del _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_undesired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#El algoritmo recibe los productos como xarrays en variablles llamadas xarr0, xarr1, xarr2... \n",
    "xarr0=kwargs[\"xarr0\"]\n",
    "del kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xarr0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo debe ser auto contenido. Puede importar y usar las librerías disponibles en CDCol, por ejemplo: \n",
    "\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- xarray\n",
    "- pycurl\n",
    "- pandas\n",
    "- nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"{}_{}_{}\".format(algorithm,version,execID))\n",
    "\n",
    "_data = dc.load(product=\"FNF_COL_UTM\", longitude=(min_long, max_long), latitude=(min_lat, max_lat), time=tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data[\"fnf_mask\"].values[0]==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compuesto temporal de medianas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "medians={} \n",
    "for band in bands:\n",
    "    datos=xarr0[band].values\n",
    "    allNan=~np.isnan(datos) #Una mascara que indica qué datos son o no nan. \n",
    "    if normalized: #Normalizar, si es necesario.\n",
    "        #Para cada momento en el tiempo obtener el promedio y la desviación estándar de los valores de reflectancia\n",
    "        m=np.nanmean(datos.reshape((datos.shape[0],-1)), axis=1)\n",
    "        st=np.nanstd(datos.reshape((datos.shape[0],-1)), axis=1)\n",
    "        # usar ((x-x̄)/st) para llevar la distribución a media 0 y desviación estándar 1, \n",
    "        # y luego hacer un cambio de espacio para la nueva desviación y media. \n",
    "        datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "    #Calcular la mediana en la dimensión de tiempo \n",
    "    medians[band]=np.nanmedian(datos,0) \n",
    "    #Eliminar los valores que no cumplen con el número mínimo de pixeles válidos dado. \n",
    "    medians[band][np.sum(allNan,0)<minValid]=np.nan\n",
    "    medians[band][_data[\"fnf_mask\"].values[0]==1]=np.nan\n",
    "del datos\n",
    "del _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_coords=xarr0.coords\n",
    "_crs=xarr0.crs\n",
    "del xarr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(train_data_path) if f.endswith('.shp')]\n",
    "classes = [f.split('.')[0] for f in files]\n",
    "shapefiles = [os.path.join(train_data_path, f) for f in files if f.endswith('.shp')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows, cols = medians[bands[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(originX, pixelWidth, 0, originY, 0, pixelHeight)\n",
    "geo_transform=(_coords[\"longitude\"].values[0], 0.000269995,0, _coords[\"latitude\"].values[0],0,-0.000271302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proj=_crs.wkt\n",
    "#proj='GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_pixels = rasterizar_entrenamiento(shapefiles, rows, cols, geo_transform, proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_train = np.nonzero(labeled_pixels)\n",
    "training_labels = labeled_pixels[is_train]\n",
    "nmed=None\n",
    "bands_data=[]\n",
    "for band in bands: \n",
    "    bands_data.append(medians[band])\n",
    "bands_data = np.dstack(bands_data)\n",
    "#training_samples = nmed[is_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows, cols, n_bands = bands_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_train = np.nonzero(labeled_pixels)\n",
    "training_labels = labeled_pixels[is_train]\n",
    "training_samples = bands_data[is_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_msk=np.sum(np.isfinite(training_samples),1)>0\n",
    "training_samples= training_samples[_msk,:]\n",
    "training_labels=training_labels[_msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_jobs=-1, n_estimators=250, verbose=1)\n",
    "classifier.fit(training_samples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier,'modelo2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = rows*cols\n",
    "flat_pixels = bands_data.reshape((n_samples, n_bands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_msk=np.sum(np.isfinite(flat_pixels),1)>0\n",
    "print _msk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = classifier.predict(flat_pixels[_msk])\n",
    "a=np.empty(rows*cols)\n",
    "a[:]=np.nan\n",
    "a[_msk]=result\n",
    "classification = a.reshape((rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#exportar(\"salida-class.tiff\", classification, geo_transform, proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar la salida\n",
    "La salida de los algoritmos puede expresarse como: \n",
    "- un xarray llamado output (que debe incluir entre sus atributos el crs del sistema de coordenadas)\n",
    "- un diccionario con varios xarray llamado `outputs`\n",
    "- Texto, en una variable llamada `outputtxt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in _coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, _coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=_coords[x]\n",
    "        \n",
    "variables={}\n",
    "#variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords)\n",
    "#             for k, v in medians.items()}\n",
    "variables[\"classification\"]=xr.DataArray(classification,dims=xdims,coords=ncoords)\n",
    "output=xr.Dataset(variables, attrs={'crs':_crs})\n",
    "\n",
    "for x in output.coords:\n",
    "    _coords[x].attrs[\"units\"]=_coords[x].units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar la salida\n",
    "La tarea genérica se encarga de generar los archivos de salida en la carpeta adecuada. \n",
    "\n",
    "__Nota__: A diferencia de la tarea genérica, que maneja los 3 tipos de salida descritos en la sección anterior, este cuaderno sólo guarda la salida definida en output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.storage import netcdf_writer\n",
    "from datacube.model import Variable, CRS\n",
    "print \"{}_{}_{}.nc\".format(algorithm,version,execID)\n",
    "nco=netcdf_writer.create_netcdf(\"{}_{}_{}.nc\".format(algorithm,version,execID))\n",
    "cords=('latitude', 'longitude','time')\n",
    "for x in cords:\n",
    "    if(x!=\"time\"):\n",
    "        netcdf_writer.create_coordinate(nco, x, _coords[x].values, _coords[x].units)\n",
    "netcdf_writer.create_grid_mapping_variable(nco, _crs)\n",
    "var= netcdf_writer.create_variable(nco, \"classification\", Variable(np.dtype(np.int32), None, ('latitude', 'longitude'), None) ,set_crs=True)\n",
    "var[:] = netcdf_writer.netcdfy_data(classification)\n",
    "nco.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
